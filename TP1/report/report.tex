\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{anysize}
\usepackage{amsmath}

\usepackage{hyperref}
\hypersetup{
	pdftitle = {IS - TP1}
	,pdfauthor = {Alexandre Santos \& João Ferreira\\ Departamento de Engenharia Informática\\ Universidade De Coimbra\\ \texttt{acs@student.dei.uc.pt |  jpbat@student.dei.uc.pt}}
	,pdfborder = {0 0 0}
}

\title{Enterprise Application Integration \\ Practical Assignment 1}
\author{Alexandre Santos \& João Ferreira\\
		Departamento de Engenharia Informática\\
		Universidade de Coimbra\\
		\texttt{acs@student.dei.uc.pt | jpbat@student.dei.uc.pt}\\
		\texttt{2009127404 | 2009113274}}
\date{October 2013}

\marginsize{3.5cm}{3.5cm}{3cm}{3cm}

\begin{document}
\maketitle
\clearpage

\tableofcontents

\setlength{\parindent}{1cm}
\setlength{\parskip}{0.3cm}

\clearpage
\section{Introduction}
\indent \indent Sometimes we need to get data from some important information providers that either don't own their official web API, or if they do its very complicated to use. With this the effort off of using it to build a simple non interactive application doesn't countervail.

In order to surpass this barrier, sometimes developer need to build their own ``APIs'', some kind of app that gives the wanted information without having to register any app on a developers plataform, or use third party non-official APIs, that might not be safe.

In the scope of Enterprise Application Integration that was asked us. We nedded to build something like that. A Web Crawler that fetches information from various pages within \href{http://www.imdb.com}{IMDb website}, and showes it in various ways, accordingly to what was asked by the professor.

The applications were developed trying to abstract the code in such way that change websites was a simple task, so that anyone with some HTML and CSS knowledge could do it.

\clearpage
\section{Architecture}
\indent \indent Like it was suggested by the professor, we used the architecture presented in the assignment, so we built three separated applications that communicate between them using JBoss.

The Web Crawler uses JSoup to fetch all the data from the websites.

The HTML Summary Creator uses Marshall to populate Java classes with the XML information and XSL to produce a pretty website.

The Stats Producer uses Unmarshall to populate Java classes with the XML information.

The three apps communicate exchanging messages that are formated in XML, like it can be seen on figure 1.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{architecture.png}
	\caption{System architecture.}
\end{figure}

\clearpage
\section{XML Schema}
\indent \indent The \emph{root element} of the document is \texttt{movie\_list}. It has a sequence of nodes \texttt{movie} as its \texttt{childs}.
Each \texttt{movie} node owns nine attributes:
\begin{description}
	\item [Name] \{\textbf{xs:string}\}
		Movie name.
	\item [Poster] \{\textbf{xs:anyURI}\}
		Movie Poster.
	\item [Score] \{\textbf{xs:double}\}
		Movie score according to IMDb user votes.
	\item [Duration] \{\textbf{xs:string}\}
		Movie duration in minutes.
	\item [Genres] \{\textbf{xs:sequence}\}
		An array of Genres. Each one is a string.
	\item [Main actors (stars)] \{\textbf{xs:sequence}\}
		An array of Stars. Each one is a string.
	\item [Director] \{\textbf{xs:string}\}
		Movie director.
	\item [Launch date] \{\textbf{xs:string}\}
		Movie launch date in United States.
	\item [Description] \{\textbf{xs:string}\}
		Movie small overview.
\end{description}

\clearpage
\section{Work Flow}
\indent \indent We started by exploring the IMDb website and trying to extract all the information we wanted. Then we noticed that instead of getting all the information from just one page we could get much more detailed information from the film webpage, so we decided to ``pay'' the price of being always fetching new webpages to increase portability. Because of that decision we can parse new urls quite easly.\footnote{On the 15th the IMDb changed the design for the top250 page, but as our crawler was so abstract the only thing we needed to do was change the filter (half line of code) and everything was working.}

Then we created a sample XML file with all the data we wanted of three movies so that we could use \texttt{trang} to create an XSD file to validate our data.

Then we used \texttt{xjc} to generate Java classes from the newly created XSD file. Wich we populated later using Jaxb Marshall.

That was the initial setup for the service. Each app has his function that will be succinctly explained, so that anyone that reads this report could easly implement the project.

\subsection{Web Crawler}
\indent \indent Our Web Crawler only needs a tuple of strings to change to a new website. The first one is the link to the webpage we wish to parse and the second one is the filter we need to apply so that in the end the result is a list of HTMLElements with the tag ``a'' with the \texttt{href} to the movie page.\\
\\Example:\\url: ``http://www.imdb.com/movies-coming-soon/2013-09/''\\filter: ``\#main td h4 a''

After getting that tuple, we get the webpage using Jsoup and the list of movies that we will analyze. Like mentioned before, in each movie we get the name, the url to the movie poster, the score, the duration, the genres, the main actorsm the director, the launch date and finally the description.

Afterwards the IMDBCrawler dumps the data into a XML formatted string to post on JMS topic (the message is not really posted but added to a queue). Then it gets back to the main menu for the user to choose another option.

\clearpage
\subsection{Receiver Apps}
\indent \indent This application creates a subscription to a JMS topic and is continuously waiting to receive messages.

Those messages are XML strings dumped by the web crawler.
\subsubsection{HTML Summary Creator}
\indent \indent In this app the message is used to create a XML file with all the information obtained. That file is used together with the XSL to create an HTML file so that the results can be interpreted by browsers that don't have a XSL engine, like Google Chrome.

As the professor requested us to avoy overwriting of the files, every output file contains the timestamp of its creation on the name.

\subsubsection{Stats Producer}
\indent \indent This app creates a top of N movies with the best score of all the movies that were analyzed in that session.

\section{Fail Over}
\indent \indent To prevent the failure when posting messages in JMS topic, we created a background running thread.

This thread first assures that exists communication with JBoss, after that keeps getting all the data that is added to a queue and tries to post it on the JMS topic.

If by some reason the application is closed and there are still some elements on the queue, a warning is showed.

\clearpage
\section{How To Run}
\indent \indent As the both of us are huge \texttt{C} fans and Java project are always tricky to compile and run, we figgured out that a Makefile would be a funny solution.

So to run this project everything you need is to put a link to your jboss folder in the root of the project, where the folders lib, src and output are.

After that these are the options:
\begin{description}
	\item [make]
		Compiles all java classes.
	\item [make clean]
		Deletes all *.class files.
	\item [make clean-out]
		Deletes all output files.
	\item [make clean-all]
		Deletes all output and *.class files.
	\item [make crawler]
		Runs the IMDb crawler application.
	\item [make stats]
		Runs the Stats Producer application.
	\item [make html]
		Runs the HTML Summary Creator application.
	\item [make jboss]
		Starts the jboss with the correct configuration.
	\item [make help]
		Shows the available commands.
\end{description}

\subsection{IMDB Crawler}
\indent \indent In this application a menu appear in wich the user can choose to get the data from any of three pre-made filters or use its website and respective filter.

There is also an option called ``Kill apps'' that sends a special message that orders that the receiver apps (HTML Summary Creator and Stats Producer) to shutdown. We choose to do this because otherwise the apps would be continuously running, wich would be a problem when we would try to connect them again, because there would still exist a connection made with the respective client id, we could also had used a listener, but this solution was simpler and faster to develop.

\subsection{Stats Producer}
\indent \indent The only interaction with this app occurs in the beginning, where the app ask wich is the N (size of the top) that user wishes to use.

\clearpage
\section{Conclusion}
\indent \indent With this project we understood that if a service doesn't offer an official API we can still easly use it.

We also learned the importance of abstracting the communication between applications, even if they are not written in the same language, because with this we can easly create applications in wich language is easier.

We found some dificulties that were easly surpassed when we read either the documentation for Jboss or the course blog or read something in the Jboss community webpage.

For that and also because we developed every feature requested, we belive that this project was a success.

\clearpage
\begin{thebibliography}{9}
\bibitem{eai blog}
	\href{http://eai-course.blogspot.pt/}{Professor Filipe Araújo blog}.

\bibitem{jQuery}
	\href{http://jquery.com/}{jQuery website}.

\bibitem{Twitter Bootstrap}
	\href{http://getbootstrap.com/}{Twitter's Bootstrap website}.

\bibitem{W3Schools}
	\href{http://www.w3schools.com/}{W3Schools website}.
	
\bibitem{jboss community}
	\href{https://community.jboss.org/}{JBoss community}.
\end{thebibliography}
\end{document}

