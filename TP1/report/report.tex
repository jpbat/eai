\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{anysize}
\usepackage{amsmath}

\usepackage{hyperref}
\hypersetup{
	pdftitle = {IS - TP1}
	,pdfauthor = {Alexandre Santos \& João Ferreira\\ Departamento de Engenharia Informática\\ Universidade De Coimbra\\ \texttt{acs@student.dei.uc.pt |  jpbat@student.dei.uc.pt}}
	,pdfborder = {0 0 0}
}

\title{Enterprise Application Integration \\ Practical Assignment 1}
\author{Alexandre Santos \& João Ferreira\\
		Departamento de Engenharia Informática\\
		Universidade de Coimbra\\
		\texttt{acs@student.dei.uc.pt | jpbat@student.dei.uc.pt}\\
		\texttt{2009127404 | 2009113274}}
\date{October 2013}

\marginsize{3.5cm}{3.5cm}{3cm}{3cm}

\begin{document}
\maketitle

\clearpage

\tableofcontents
\clearpage

\setlength{\parindent}{1cm}
\setlength{\parskip}{0.3cm}

\section{Introduction}
\indent \indent Sometimes we need to get data from some good information providers that either don't own their official web API, or if they do its very complicated to use. With this the effort off of using it to build a simple non interactive application doesn't countervail.

In order to surpass this barrier, developer need to develop their own ``APIs'', build some kind of app that gives the wanted information without having to register any app on a developers plataform, or use third party non-official APIs, that might not be safe.

In the scope of Enterprise Application Integration that was asked us. We nedded to build something like that. A Web Crawler that fetches information from various pages within \href{http://www.imdb.com}{IMDb website}, and showes it in various ways, accordingly to what was asked by the professor.

The main application was developed trying to abstract the code in such way that change websites was a simple task, that anyone with some HTML and CSS knowledge could do.

\section{Architecture}
\indent \indent Like it was suggested by the professor, we used the architecture presented in the assignment, so we built three separated applications that communicate between them using JBoss.

The Web Crawler uses JSoup to fetch all the data from the websites.

The HTML Summary Creator uses Marshall to populate Java classes with the XML information.

The Stats Producer uses Unmarshall to populate Java classes with the XML information.

The three apps communicate exchanging messages that are formated in XML, like it can be seen on figure 1.

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{architecture.png}
	\caption{System architecture.}
\end{figure}
\clearpage


\section{Work Flow}
\indent \indent We started by exploring the IMDb website and trying to extract all the information we wanted. Then we noticed that instead of getting all the information from just one page we could get much more detailed information from the film webpage, so we decided to ``pay'' the price of being always fetching webpages to increase portability. Because of that decision we can parse new urls quite easly.

Then we created a sample XML file with all the data we wanted of three movies. With that sample and using \texttt{trang} we created a XSD file to validate our data.

Then we used \texttt{trang} to generate Java classes from the newly created XSD file. Wich we populated later using Jaxb Marshall.

That was the initial setup for the service. Each app has his function that will be succinctly explained, so that anyone that reads this report could easly implement the project.

\subsection{Web Crawler}
\indent \indent Our Web Crawler only needs a tuple of strings to change to a new website. The first one is the link to the webpage we wish to parse and the second one is the filter we need to apply so that in the end the result is a list of HTMLElements with the tag ``a'' with the \texttt{href} to the movie page.\\
\\Example:\\url: ``http://www.imdb.com/movies-coming-soon/2013-09/''\\filter: ``\#main td h4 a''

After getting that tuple, we get the webpage using Jsoup and the list of movies that we will analyze.

Afterwards the IMDBCrawler dumps the data into a XML formatted string to post on JMS topic. Then it gets back to the main menu for the user to choose another option.

\subsection{HTML Summary Creator}
\indent \indent This application creates a subscription to a JMS topic and is continuously waiting to receive messages.

Those messages are XML strings dumped by the web crawler, that are used to create a XML file with all the information obtained. That file is used together with the XSL to create an HTML file so that the results can be interpreted by browsers that don't have a XSL engine, like Google Chrome.

As the professor requested us to avoy overwriting of the files, every output file contains the timestamp of its creation on the name.
\clearpage

\subsection{Stats Producer}
\indent \indent

\section{Fail Over}
\indent \indent

\section{How To}
\indent \indent

\section{Conclusion}
\indent \indent 
\clearpage

\begin{thebibliography}{9}
\bibitem{jQuery}
	\href{http://jquery.com/}{jQuery website}.

\bibitem{Twitter Bootstrap}
	\href{http://getbootstrap.com/}{Twitter's Bootstrap website}.

\bibitem{W3Schools}
	\href{http://www.w3schools.com/}{W3Schools website}.

\end{thebibliography}
\end{document}